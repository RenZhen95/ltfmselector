{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning (DQN) Tutorial\n",
    "=====================================\n",
    "\n",
    "**Author**: [Adam Paszke](https://github.com/apaszke)\n",
    "\n",
    ":   [Mark Towers](https://github.com/pseudo-rnd-thoughts)\n",
    "\n",
    "This tutorial shows how to use PyTorch to train a Deep Q Learning (DQN)\n",
    "agent on the CartPole-v1 task from\n",
    "[Gymnasium](https://gymnasium.farama.org).\n",
    "\n",
    "You might find it helpful to read the original [Deep Q Learning\n",
    "(DQN)](https://arxiv.org/abs/1312.5602) paper\n",
    "\n",
    "**Task**\n",
    "\n",
    "The agent has to decide between two actions - moving the cart left or\n",
    "right - so that the pole attached to it stays upright. You can find more\n",
    "information about the environment and other more challenging\n",
    "environments at [Gymnasium\\'s\n",
    "website](https://gymnasium.farama.org/environments/classic_control/cart_pole/).\n",
    "\n",
    "[//]: #![CartPole](https://pytorch.org/tutorials/_static/img/cartpole.gif)\n",
    "\n",
    "As the agent observes the current state of the environment and chooses\n",
    "an action, the environment *transitions* to a new state, and also\n",
    "returns a reward that indicates the consequences of the action. In this\n",
    "task, rewards are +1 for every incremental timestep and the environment\n",
    "terminates if the pole falls over too far or the cart moves more than\n",
    "2.4 units away from center. This means better performing scenarios will\n",
    "run for longer duration, accumulating larger return.\n",
    "\n",
    "The CartPole task is designed so that the inputs to the agent are 4 real\n",
    "values representing the environment state (position, velocity, etc.). We\n",
    "take these 4 inputs without any scaling and pass them through a small\n",
    "fully-connected network with 2 outputs, one for each action. The network\n",
    "is trained to predict the expected value for each action, given the\n",
    "input state. The action with the highest expected value is then chosen.\n",
    "\n",
    "**Packages**\n",
    "\n",
    "First, let\\'s import needed packages. Firstly, we need\n",
    "[gymnasium](https://gymnasium.farama.org/) for the environment,\n",
    "installed by using [pip]{.title-ref}. This is a fork of the original\n",
    "OpenAI Gym project and maintained by the same team since Gym v0.19. If\n",
    "you are running this in Google Colab, run:\n",
    "\n",
    "``` {.bash}\n",
    "%%bash\n",
    "pip3 install gymnasium[classic_control]\n",
    "```\n",
    "\n",
    "We\\'ll also use the following from PyTorch:\n",
    "\n",
    "-   neural networks (`torch.nn`)\n",
    "-   optimization (`torch.optim`)\n",
    "-   automatic differentiation (`torch.autograd`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liaw/repo/ltfmselector/.venv/lib/python3.13/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import os, sys\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "plt.rcParams['font.serif'] = ['CMU Serif']\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['mathtext.fontset'] = 'cm'\n",
    "plt.rcParams['text.latex.preamble'] = r'\\usepackage{amsmath}'\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "# https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py\n",
    "\n",
    "# Set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "# My GPU is too old\n",
    "device = \"cpu\"\n",
    "\n",
    "# Control randomness (04.11.2025:: Doesn't work as planned)\n",
    "seed_value = 12\n",
    "# Set seed for Python's built-in random module\n",
    "random.seed(seed_value)\n",
    "# Set seed for NumPy's RNG\n",
    "np.random.seed(seed_value)\n",
    "# Set seeds for PyTorch (CPU and all CUDA devices)\n",
    "torch.manual_seed(seed_value)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value) # for multi-GPU setups\n",
    "# For some environments, setting the hash seed is also recommended\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay Memory\n",
    "=============\n",
    "\n",
    "We\\'ll be using experience replay memory for training our DQN. It stores\n",
    "the transitions that the agent observes, allowing us to reuse this data\n",
    "later. By sampling from it randomly, the transitions that build up a\n",
    "batch are decorrelated. It has been shown that this greatly stabilizes\n",
    "and improves the DQN training procedure.\n",
    "\n",
    "For this, we\\'re going to need two classes:\n",
    "\n",
    "-   `Transition` - a named tuple representing a single transition in our\n",
    "    environment. It essentially maps (state, action) pairs to their\n",
    "    (next\\_state, reward) result, with the state being the screen\n",
    "    difference image as described later on.\n",
    "-   `ReplayMemory` - a cyclic buffer of bounded size that holds the\n",
    "    transitions observed recently. It also implements a `.sample()`\n",
    "    method for selecting a random batch of transitions for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN algorithm\n",
    "=============\n",
    "\n",
    "Our environment is deterministic, so all equations presented here are\n",
    "also formulated deterministically for the sake of simplicity. In the\n",
    "reinforcement learning literature, they would also contain expectations\n",
    "over stochastic transitions in the environment.\n",
    "\n",
    "Our aim will be to train a policy that tries to maximize the discounted,\n",
    "cumulative reward\n",
    "$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$, where $R_{t_0}$\n",
    "is also known as the *return*. The discount, $\\gamma$, should be a\n",
    "constant between $0$ and $1$ that ensures the sum converges. A lower\n",
    "$\\gamma$ makes rewards from the uncertain far future less important for\n",
    "our agent than the ones in the near future that it can be fairly\n",
    "confident about. It also encourages agents to collect reward closer in\n",
    "time than equivalent rewards that are temporally far away in the future.\n",
    "\n",
    "The main idea behind Q-learning is that if we had a function\n",
    "$Q^*: State \\times Action \\rightarrow \\mathbb{R}$, that could tell us\n",
    "what our return would be, if we were to take an action in a given state,\n",
    "then we could easily construct a policy that maximizes our rewards:\n",
    "\n",
    "$$\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)$$\n",
    "\n",
    "However, we don\\'t know everything about the world, so we don\\'t have\n",
    "access to $Q^*$. But, since neural networks are universal function\n",
    "approximators, we can simply create one and train it to resemble $Q^*$.\n",
    "\n",
    "For our training update rule, we\\'ll use a fact that every $Q$ function\n",
    "for some policy obeys the Bellman equation:\n",
    "\n",
    "$$Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))$$\n",
    "\n",
    "The difference between the two sides of the equality is known as the\n",
    "temporal difference error, $\\delta$:\n",
    "\n",
    "$$\\delta = Q(s, a) - (r + \\gamma \\max_a' Q(s', a))$$\n",
    "\n",
    "To minimize this error, we will use the [Huber\n",
    "loss](https://en.wikipedia.org/wiki/Huber_loss). The Huber loss acts\n",
    "like the mean squared error when the error is small, but like the mean\n",
    "absolute error when the error is large - this makes it more robust to\n",
    "outliers when the estimates of $Q$ are very noisy. We calculate this\n",
    "over a batch of transitions, $B$, sampled from the replay memory:\n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n",
    "  \\frac{1}{2}{\\delta^2}  & \\text{for } |\\delta| \\le 1, \\\\\n",
    "  |\\delta| - \\frac{1}{2} & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "\\end{aligned}$$\n",
    "\n",
    "Q-network\n",
    "---------\n",
    "\n",
    "Our model will be a feed forward neural network that takes in the\n",
    "difference between the current and previous screen patches. It has two\n",
    "outputs, representing $Q(s, \\mathrm{left})$ and $Q(s, \\mathrm{right})$\n",
    "(where $s$ is the input to the network). In effect, the network is\n",
    "trying to predict the *expected return* of taking each action given the\n",
    "current input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "========\n",
    "\n",
    "Hyperparameters and utilities\n",
    "-----------------------------\n",
    "\n",
    "This cell instantiates our model and its optimizer, and defines some\n",
    "utilities:\n",
    "\n",
    "-   `select_action` - will select an action according to an epsilon\n",
    "    greedy policy. Simply put, we\\'ll sometimes use our model for\n",
    "    choosing the action, and sometimes we\\'ll just sample one uniformly.\n",
    "    The probability of choosing a random action will start at\n",
    "    `EPS_START` and will decay exponentially towards `EPS_END`.\n",
    "    `EPS_DECAY` controls the rate of the decay.\n",
    "-   `plot_durations` - a helper for plotting the duration of episodes,\n",
    "    along with an average over the last 100 episodes (the measure used\n",
    "    in the official evaluations). The plot will be underneath the cell\n",
    "    containing the main training loop, and will update after every\n",
    "    episode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n # >> 2\n",
    "\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)    # >> 4\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random() # Sample a random number [0.0, 1.0]\n",
    "\n",
    "    # A threshold which exponentially decreases\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "\n",
    "    steps_done += 1\n",
    "\n",
    "    # If the randomly sampled number is less than threshold ::\n",
    "    # Use the policy network to select action that maximizes Q-value\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            \n",
    "            # indices.view(1, 1) to pick action (index) with the larger \n",
    "            # expected reward.\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "        \n",
    "    # Else ::\n",
    "    # Select a random action\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the exponential decay of $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x320 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(5.0, 3.2))\n",
    "\n",
    "EPS_DECAY1 = 1000\n",
    "EPS_DECAY2 = 700\n",
    "EPS_DECAY3 = 500\n",
    "\n",
    "x_data = np.linspace(0, 5000, 5000)\n",
    "eps1 = EPS_END + (EPS_START - EPS_END) * np.exp(-1. * x_data / EPS_DECAY1)\n",
    "eps2 = EPS_END + (EPS_START - EPS_END) * np.exp(-1. * x_data / EPS_DECAY2)\n",
    "eps3 = EPS_END + (EPS_START - EPS_END) * np.exp(-1. * x_data / EPS_DECAY3)\n",
    "\n",
    "y_tick_locations = [EPS_END, EPS_START]\n",
    "y_tick_labels = [r\"${\\epsilon}_{\\mathrm{end}}$\", r\"${\\epsilon}_{\\mathrm{start}}$\"]\n",
    "\n",
    "ax.plot(x_data, eps1, label=r\"${\\epsilon}_{\\mathrm{decay}}=1000$\", linewidth=1.5)\n",
    "ax.plot(x_data, eps2, label=r\"${\\epsilon}_{\\mathrm{decay}}=700$\", linewidth=1.5)\n",
    "ax.plot(x_data, eps3, label=r\"${\\epsilon}_{\\mathrm{decay}}=500$\", linewidth=1.5)\n",
    "\n",
    "# Use set_xticks/set_yticks and pass both the locations and the labels\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks(y_tick_locations, labels=y_tick_labels)\n",
    "\n",
    "# Optional: Add gridlines ONLY at the custom tick locations\n",
    "ax.grid(linestyle='--', linewidth=2.0)\n",
    "\n",
    "# 5. Final Plot Customization\n",
    "ax.set_title(r\"Probability that agent selects a random action $\\epsilon$\", loc='left', fontsize='large')\n",
    "\n",
    "ax.set_xlabel(\"Cumulative time-step $t_c$\")\n",
    "fig.legend(loc='upper right', bbox_to_anchor=(0.97, 0.85))\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"EpsDecay.eps\", format=\"eps\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop\n",
    "=============\n",
    "\n",
    "Finally, the code for training our model.\n",
    "\n",
    "Here, you can find an `optimize_model` function that performs a single\n",
    "step of the optimization. It first samples a batch, concatenates all the\n",
    "tensors into a single one, computes $Q(s_t, a_t)$ and\n",
    "$V(s_{t+1}) = \\max_a Q(s_{t+1}, a)$, and combines them into our loss. By\n",
    "definition we set $V(s) = 0$ if $s$ is a terminal state. We also use a\n",
    "target network to compute $V(s_{t+1})$ for added stability. The target\n",
    "network is updated at every step with a [soft\n",
    "update](https://arxiv.org/pdf/1509.02971.pdf) controlled by the\n",
    "hyperparameter `TAU`, which was previously defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # [\n",
    "    #    Transition #1: (state, action, next_state, reward), \n",
    "    #    Transition #2: (state, action, next_state, reward), \n",
    "    #    ..., \n",
    "    #    Transition #BATCH_SIZE: (state, action, next_state, reward)\n",
    "    # ]\n",
    "\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    # [\n",
    "    #    state: (#1, #2, ..., #BATCH_SIZE),\n",
    "    #    action: (#1, #2, ..., #BATCH_SIZE),\n",
    "    #    next_state: (#1, #2, ..., #BATCH_SIZE),\n",
    "    #    reward: (#1, #2, ..., #BATCH_SIZE)\n",
    "    # ]\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    #  - A final state is the state, right before the environment terminates\n",
    "    #  - next_state is None if environment terminates (see L16 in next cell)\n",
    "    non_final_mask = torch.tensor(\n",
    "        tuple(\n",
    "            map(lambda s: s is not None, batch.next_state)\n",
    "        ), device=device, dtype=torch.bool\n",
    "    )\n",
    "    # [True True False True ... True False]\n",
    "\n",
    "    # Batch of non-final next_states of tensor dimensions: (<#BATCH_SIZE, 4)\n",
    "    non_final_next_states = torch.cat(\n",
    "        [s for s in batch.next_state if s is not None]\n",
    "    )\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken.\n",
    "    # \n",
    "    # These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s') = r + GAMMA * max_(a) {Q(s', a)} for alprint(state_action_values)l next states.\n",
    "    # - Q(s', a) computed based on \"older\" target network, selecting for\n",
    "    #   action that maximizes this term\n",
    "\n",
    "    # This is merged based on the mask, such that we'll have either:\n",
    "    #  1. the V(s') or\n",
    "    #  2. 0 (cause that state was final for that episode)\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "\n",
    "    # Compute V(s')\n",
    "    expected_state_action_values = reward_batch + (next_state_values * GAMMA)\n",
    "\n",
    "    # Compute loss\n",
    "    criterion = nn.MSELoss()\n",
    "    # criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "    Q_avr = state_action_values.detach().numpy().mean()\n",
    "    r_avr = reward_batch.unsqueeze(1).numpy().mean()\n",
    "    V_avr = expected_state_action_values.unsqueeze(1).numpy().mean()\n",
    "\n",
    "    return (Q_avr, r_avr, V_avr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you can find the main training loop. At the beginning we reset\n",
    "the environment and obtain the initial `state` Tensor. Then, we sample\n",
    "an action, execute it, observe the next state and the reward (always 1),\n",
    "and optimize our model once. When the episode ends (our model fails), we\n",
    "restart the loop.\n",
    "\n",
    "You should see the model constantly achieve 500 steps within\n",
    "600 training episodes. Training RL agents can be a noisy process, so\n",
    "restarting training can produce better results if convergence is not\n",
    "observed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1, figsize=(7, 4.8))\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if not show_result:\n",
    "        plt.clf()\n",
    "\n",
    "    plt.title('Duration (time-steps) of pendulum kept balanced')\n",
    "    plt.xlabel(r'Episode $k$')\n",
    "    plt.ylabel(r'Total time-steps, per epsiode $t$')\n",
    "    plt.plot(durations_t.numpy(), label=\"Time-steps\")\n",
    "\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy(), label=r\"Moving average of time-steps (100 episodes)\")\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "    plt.grid()\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.97, 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n"
     ]
    }
   ],
   "source": [
    "# My personal parameter\n",
    "num_episodes = 750\n",
    "\n",
    "total_time_steps = 0\n",
    "\n",
    "Q_avr_list = []\n",
    "r_avr_list = []\n",
    "V_avr_list = []\n",
    "\n",
    "print(\"Training ...\")\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        total_time_steps += 1\n",
    "\n",
    "        output = optimize_model()\n",
    "        if not output is None:\n",
    "            Q_avr_list.append(output[0])\n",
    "            r_avr_list.append(output[1])\n",
    "            V_avr_list.append(output[2])\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        #     + (1  )\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            # plot_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the training progression\n",
    "### Time-steps pendulum is kept upright over training episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training progression\n",
    "fig, ax = plt.subplots(1, figsize=(6.25, 3.5))\n",
    "\n",
    "durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "\n",
    "ax.set_title('Duration (time-steps) of pendulum kept balanced', loc='left', fontsize=\"large\")\n",
    "ax.set_xlabel(r'Episode $k$')\n",
    "ax.set_ylabel(r'Total time-steps $t$')\n",
    "ax.plot(durations_t.numpy(), label=\"Time-steps\")\n",
    "\n",
    "# Take 100 episode averages and plot them too\n",
    "if len(durations_t) >= 100:\n",
    "    means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "    means = torch.cat((torch.zeros(99), means))\n",
    "    plt.plot(means.numpy(), label=r\"Moving average of time-steps (100 episodes)\")\n",
    "\n",
    "plt.grid()\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.005, 0.25))\n",
    "plt.show()\n",
    "fig.savefig(\"PendulumDuration.eps\", format=\"eps\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progression of selected actions by a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store selected actions\n",
    "actions = []\n",
    "\n",
    "# Initialize the environment and get its state\n",
    "state, info = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "for t in count():\n",
    "    action = select_action(state)\n",
    "    actions.append(action.item())\n",
    "\n",
    "    observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    if terminated:\n",
    "        next_state = None\n",
    "    else:\n",
    "        next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "    memory.push(state, action, next_state, reward)\n",
    "\n",
    "    # Move to the next state\n",
    "    state = next_state\n",
    "        \n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the sequence of actions taken\n",
    "fig, ax = plt.subplots(1, figsize=(6.5, 3.5))\n",
    "\n",
    "ax.set_title('Sequence of actions of trained agent', loc='left', fontsize=\"medium\")\n",
    "ax.set_xlabel(r'Time-steps $t$')\n",
    "ax.set_ylabel(r'Action $a_t$')\n",
    "ax.plot(actions, '.')\n",
    "\n",
    "plt.grid()\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.005, 0.25))\n",
    "plt.show()\n",
    "# fig.savefig(\"ActionSeqPendulum.eps\", format=\"eps\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the average: Q(s,a) per batch of optimization\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5.5, 3.5))\n",
    "ax.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "\n",
    "# Get total number of optimization steps\n",
    "n_optimization = np.arange(0, len(Q_avr_list), 1)\n",
    "ax.plot(n_optimization, Q_avr_list)\n",
    "ax.set_title(\n",
    "    \"Average $Q$-values of sampled batch \" + \n",
    "    r\"$\\frac{1}{|\\mathcal{B}_D|} \\sum_{\\boldsymbol{e} \\in \\mathcal{B}_D}$\" +\n",
    "    r\"$\\left[Q_{\\boldsymbol{\\pi}}\\,(\\underline{s}, a; \\boldsymbol{\\theta}) \\right]$\", \n",
    "    loc='left', fontsize=\"large\"\n",
    ")\n",
    "ax.set_ylabel(\"$Q$-Value\")\n",
    "ax.set_xlabel(\"Cumulative time-step $t_c$\")\n",
    "ax.grid()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(\"QValuesPendulum.eps\", format=\"eps\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the average:\n",
    "# 1. Q(s,a)\n",
    "# 2. r        > this should just be 1\n",
    "# 3. V(s', a')\n",
    "# per batch of optimization\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 3.8), sharey=True)\n",
    "ax[0].ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "ax[1].ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "\n",
    "# Get total number of optimization steps\n",
    "n_optimization = np.arange(0, len(Q_avr_list), 1)\n",
    "ax[0].plot(n_optimization, Q_avr_list)\n",
    "ax[0].set_title(\n",
    "    \"Average Q-values of sampled batch\\n\" + \n",
    "    r\"$\\frac{1}{|\\mathcal{B}_D|} \\sum_{\\boldsymbol{e} \\in \\mathcal{B}_D} \\left[Q_{\\boldsymbol{\\pi}}(s, a; \\boldsymbol{\\theta}) \\right]$\",\n",
    "    loc='left', fontsize=\"medium\"\n",
    ")\n",
    "ax[0].set_ylabel(r\"$Q (s, a)$\")\n",
    "ax[0].set_xlabel(\"Cumulative time-step $t_c$\")\n",
    "ax[0].grid()\n",
    "ax[1].plot(n_optimization, V_avr_list)\n",
    "ax[1].set_title(\n",
    "    \"Average target values of sampled batch\\n\" + \n",
    "    r\"$\\frac{1}{|\\mathcal{B}_D|} \\sum_{\\boldsymbol{e} \\in \\mathcal{B}_D} \\left[r + \\gamma \\max_{a'} Q(s', a'; \\boldsymbol{\\theta}^-) \\right]$\",\n",
    "    loc='left', fontsize=\"medium\"\n",
    ")\n",
    "ax[1].set_ylabel(r\"$r + \\gamma \\max_{a'} Q(s', a')$\")\n",
    "ax[1].set_xlabel(\"Cumulative time-step $t_c$\")\n",
    "ax[1].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(\"Q_TargetValuesPendulum.eps\", format=\"eps\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the $Q(s,a)$ from the trained policy network.\n",
    "\n",
    "The policy network's goal is to predict the optimal action-value function $Q^* (s,a)$, which estimates the expected return $R$ for an action $a$, given the state $s$. Once a policy network has been adequately trained to yield $Q^*(s,a)$, the optimal policy is then obtained by simply selecting the action $a$ that maximizes the return $R$, specifically via $\\max_{a} Q^* (s, a)$ for a given state $s$.\n",
    "\n",
    "Remember, the reward is $+1$ at every time-step the pole has not fallen. In other words, the return $R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$ is somewhat akin to the **total number of time-steps**. \n",
    "\n",
    "Hence, a probably more interesting interpretation of $Q^* (s, a)$ with regards to this example can be summarized as the following:\n",
    "\n",
    "> The $Q^* (s,a)$ returned by the trained policy network gives us an **estimation of how many more time-steps will the pole be kept upright if the agent takes the action, $a$ for the given state $s$**\n",
    "\n",
    "**Note:** The allowed range is only between -24 and +24. Anything beyond this range will just result in the policy network deciding on a nonsensical action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEG_TO_RAD = lambda x: x * np.pi / 180\n",
    "get_action = lambda x: \"LEFT\" if x == 0 else \"RIGHT\"\n",
    "\n",
    "def examine_policy(s):\n",
    "    # Get Q(s,a)\n",
    "    Q = policy_net(s)\n",
    "    Q_left = Q[0,0].item()\n",
    "    Q_rght = Q[0,1].item()\n",
    "    print(f\"Q(s, left) = {Q_left} | Q(s, right) = {Q_rght}\")\n",
    "\n",
    "    # Get best action\n",
    "    a_idx = Q.max(1).indices.view(1, 1)\n",
    "    a = get_action(a_idx)\n",
    "    \n",
    "    print(f\"Best action: {a}\\n\")\n",
    "\n",
    "# Pole tilted -22.5, then released\n",
    "examine_policy(torch.tensor([[0.0, 0.0, -DEG_TO_RAD(22.5), 0.0]])) # >> LEFT (expected action)\n",
    "\n",
    "# Pole tilted -22.5 with some angular velocity in anti-clockwise direction\n",
    "examine_policy(torch.tensor([[0.0, 0.0, -DEG_TO_RAD(22.5), -0.5]])) # >> LEFT (expected action)\n",
    "# - also notice how the Q-values are generally lower\n",
    "\n",
    "# Pole tilted -22.5 with some angular velocity in clockwise direction\n",
    "examine_policy(torch.tensor([[0.0, 0.0, -DEG_TO_RAD(22.5), +0.5]])) # >> LEFT (expected action)\n",
    "# - now notice how the Q-values are now much higher than even the first case\n",
    "\n",
    "# Pole tilted -22.5 with a greater angular velocity in clockwise direction\n",
    "examine_policy(torch.tensor([[0.0, 0.0, -DEG_TO_RAD(22.5), +1.5]])) # >> RIGHT (expected action)\n",
    "\n",
    "# Pole tilted +22.5, then released\n",
    "examine_policy(torch.tensor([[0.0, 0.0, +DEG_TO_RAD(22.5), 0.0]])) # >> RIGHT (expected action)\n",
    "\n",
    "# Pole tilted +22.5 with some angular velocity in clockwise direction\n",
    "examine_policy(torch.tensor([[0.0, 0.0, +DEG_TO_RAD(22.5), +0.5]])) # >> RIGHT (expected action)\n",
    "# - for some reason, the Q-values are negative? But nonetheless, Q(s,RIGHT) is much larger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
