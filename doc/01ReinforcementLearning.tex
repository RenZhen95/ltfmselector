RL is a branch of machine learning that deals with learning control laws and policies to interact with an environment, via trial-and-error to achieve a long-term objective \cite{mnih2015,sutton2018,brunton2022}. The learning is guided via feedback, or \emph{reinforcement}, a concept biologically inspired from animal psychology. Animals have been observed to be ``hardwired'' to recognize pain and hunger as negative rewards, and food intake as positive rewards, and thus, mold their behavior accordingly to best maximize (positive) rewards \cite{pavlov2010,brunton2022}. As shown in Figure \ref{fig:RLSchematic}, an \emph{agent} senses the \emph{state} of its \emph{environment}, and learns to take \emph{actions} that maximizes \emph{cumulative future rewards}.
\begin{figure}[h!]
  \centering
  \begin{overpic}[width=1.0\columnwidth]{ReinforcementLearning.eps}
    % Nouns
    \put(61, 34){\emph{ENVIRONMENT}}
    \put(2, 26){\emph{AGENT}}
    \put(1, 18.5){\emph{STATE}, $\nvec{s}$}
    \put(36, 27){\emph{POLICY}}
    \put(37, 25){$\policy$}
    \put(35, 38){\emph{REWARD}, $R$}
    % Verbs
    \put(40, 0){Observe \emph{STATE}, $\nvec{s}$}
    \put(48, 24.5){Perform}
    \put(48, 21.5){\emph{ACTION}, $a$}
    % Agent
    \put(17.5, 24){$\dot{x}$}
    \put(17.5, 21){$x$}
    \put(17.5, 17){$\varphi$}
    \put(17.5, 13){$\dot{\varphi}$}
    \put(41, 21){$+F$}
    \put(41, 16){$-F$}
    % Environment
    \put(92, 11){$x$}
    \put(66, 30){$y$}
    \put(73, 6.5){$x$}
    \put(84, 7){$\dot{x}$}
    \put(68, 12){$a=F$}
    \put(84, 29){$\varphi$}
    \put(88, 23){$\dot{\varphi}$}
    \put(65, 22){$\nvec{s}=\begin{bmatrix}x \\ \dot{x} \\ \varphi \\ \dot{\varphi}\end{bmatrix}$}
    % Parameters
    \put(90, 30){$m_p$}
    \put(86, 17){massless}
    \put(92, 14.5){rod, $\ell$}
    \put(87, 13){$m_c$}
  \end{overpic}
  \caption{Schematic of RL, where an agent senses its environmental state $\nvec{s}$ and performs an action $a$, according to a policy $\policy$ that is optimized through learning to maximize cumulative future rewards $R$. In recent works, a typical approach to represent the policy $\policy$ is to use a deep neural network. Such a policy is known as a \emph{deep policy network}. Figure adapted from \cite{brunton2022}.}
  \label{fig:RLSchematic}
\end{figure}
