The environment is formulated as an episodic partially observable Markov Decision Process (POMDP), where an agent gradually learns by autonomously exploring different combinations of feature subsets and PMs to assess a patient's gait poststroke based on the extracted gait features. At each $k$-th episode, a randomly selected measured stride-pair yields a $D$-dimensional \emph{sample} $\sample_{k}$, comprised of the extracted gait features, alongside the corresponding medical-board's gait assessment $y_k$. The \emph{feature values} $x_{k,d}$ of the feature ${\feature}_{d} \in \fset$ make up the elements of $\sample_{k}$. The environment for a given time-step $t$ of the episode $k$, is represented by the state as formulated in (\ref{eq:ltfmstate})
\begin{equation}
    \begin{split}
        \nvec{s}_{t} &= \begin{bmatrix} {\nvec{\mathtt{X}}_t}^T & {\nvec{\mathtt{F}}_t}^T & g_t \end{bmatrix}^T \\
                     &= \begin{bmatrix} \mathtt{X}_{1,t} & \mathtt{X}_{2,t} & \cdots & \mathtt{X}_{D,t} & \mathtt{F}_{1,t} & \mathtt{F}_{2,t} & \cdots & \mathtt{F}_{D,t} & \mathtt{G}_{t} \end{bmatrix}^T \,.
    \end{split}
    \label{eq:ltfmstate}
\end{equation}
The episodic index will be dropped here for purposes of brevity. The vectors $\nvec{\mathtt{X}}_t$ and $\nvec{\mathtt{F}}_t$ denote the \emph{observed values} and \emph{observed features}, respectively. The \emph{selected PM} is denoted by $\mathtt{G}_{t}$. The agent is allowed to perform actions $a \in \mathcal{A}$, where the set $\mathcal{A}$ comprises three types of actions as shown in (\ref{eq:ltfmactions})
\begin{equation}
    \mathcal{A} = \begin{Bmatrix}
      \overbrace{a_0}^{\text{make prediction}} &
      \underbrace{\begin{matrix} a_1 & a_2 & \cdots & a_D \end{matrix}}_{\text{recruit feature}} &
      \underbrace{\begin{matrix} a_{D+1} & a_{D+2} & \cdots & a_{D+|\mathcal{G}|} \end{matrix}}_{\text{select PM}}
      \end{Bmatrix} \,.
    \label{eq:ltfmactions}
\end{equation}
The action $a_0$ fits the chosen PM $g$ to the training dataset, was has been trimmed to only include the selected subset of features $\fsubset$. The fitted PM is then used to predict $\hat{y}_k$, followed by computing the absolute error between the predicted and medical-board's gait assessment $\left| \hat{y}_k - y_k \right|$, which will be used to formulate the reward function. Actions $\left\{ a_{d} \,|\, 1 \leq d \leq D \right\}$ recruit feature ${\feature}_d$ to the subset of selected features $\fsubset$, whereas actions $\left\{ a_{j} \,|\, D+1 \leq j \leq D+|\mathcal{G}| \right\}$ chooses a PM $g_{j-D} \in \pmset$. The set ${\pmset} = \begin{Bmatrix} g_1 & g_2 & \cdots & g_{|\mathcal{G}|}\end{Bmatrix}$ denote the set of possible PMs from which the agent can choose from. The dynamics of the POMDP evolve deterministically according to (\ref{eq:ltfmdynamics}).
\begin{align}
    P(\nvec{s}, a) &=
    \begin{cases}
        \mathtt{X}_{d} = x_{d} \text{ and } \mathtt{F}_{d} = 1 & \text{if } a = a_{d} \text{, where } 1 \leq d \leq D \\
        \mathtt{G} = g_{j-D} & \text{if } a = a_{j} \text{, where } D+1 \leq j \leq D+|\mathcal{G}| \\
        T_{\text{end}} & \text{if } a = a_0 \\
    \end{cases}
    \label{eq:ltfmdynamics}
\end{align}
The episode begins with no features recruited and the initial state $\nvec{s}_0$ as shown in (\ref{eq:ltfminitialstate}).
\begin{equation}
    \begin{split}
        \nvec{s}_0 &= \begin{bmatrix} {\nvec{\mathtt{X}}_0}^T & {\nvec{\mathtt{F}}_0}^T & \mathtt{G}_{0} \end{bmatrix}^T \\
                     &= \begin{bmatrix} \mathtt{X}_{1,0} & \mathtt{X}_{2,0} & \cdots & \mathtt{X}_{D,0} & \mathtt{F}_{1,0} & \mathtt{F}_{2,0} & \cdots & \mathtt{F}_{D,0} & \mathtt{G}_{0} \end{bmatrix}^T \\
                     &= \begin{bmatrix} \bar{x}_{1} & \bar{x}_{2} & \cdots & \bar{x}_{D} & 0 & 0 & \cdots & 0 & \mathtt{G}_{0} \end{bmatrix}^T \,,
    \end{split}
    \label{eq:ltfminitialstate}
\end{equation}
where $\bar{x}_d$ denotes the average feature value of the feature ${\feature}_d$ computed from the training dataset. The observed feature $\mathtt{F}_d$ assumes the value $0$ to indicate feature's ${\feature}_d$ exclusion from the set of selected features $\fsubset$, and the PM $\mathtt{G}_{0}$ is initialized with a randomly chosen prediction model $g$ from the set $\mathcal{G}$. Consider a POMDP comprised of three features and two options of PMs as an example. At the current time-step $t={\xi}$, the recruited features include $\fsubset = \left\{ {\feature}_{1} \,, {\feature}_{3} \right\}$, and the PM $\mathtt{G}_{\xi} = g_1$ chosen. This would consequently yield the state $\nvec{s}_{\xi}$
\begin{equation}
    \begin{split}
        \nvec{s}_{\xi} &= \begin{bmatrix} \mathtt{X}_{1,\xi} & \mathtt{X}_{2,\xi} & \mathtt{X}_{3,\xi} & \mathtt{F}_{1,\xi} & \mathtt{F}_{2,\xi} & \mathtt{F}_{3,\xi} & \mathtt{G}_{\xi} \end{bmatrix}^T \\
                     &= \begin{bmatrix} x_{1} & \bar{x}_{2} & x_{3} & 1 & 0 & 1 & g_1 \end{bmatrix}^T \,.
    \end{split}
    \label{eq:ltfmexamplestate}
\end{equation}
The reward function is as given in (\ref{eq:ltfmreward}).
\begin{align}
    R(\nvec{s}, a) &=
    \begin{cases}
        -\lambda c \left( {\feature}_{d} \right) & \text{if } a = a_{d} \text{, where } 1 \leq d \leq D \\
        -c_{\fsubset} & \text{if } a = a_{d} \in \fsubset \text{, where } 1 \leq d \leq D \\
        -\lambda_{g} c_{g} \left( g_{j-D} \right) & \text{if } a = a_{j} \text{, where } D+1 \leq j \leq D+|\mathcal{G}| \\
        0 & \text{if } a = a_0 \text{, with } \left| \hat{y}_k - y_k \right| < \Delta \\
        -\left| \hat{y}_k - y_k \right| & \text{if } a = a_0 \text{, with } \left| \hat{y}_k - y_k \right| \geq \Delta \\
        -c_{g, \emptyset} & \text{if } a = a_0 \text{, with } \fsubset = \emptyset
    \end{cases}
    \label{eq:ltfmreward}
\end{align}
The penalties of recruiting a feature ${\feature}_d$ and choosing a PM $g_{j-D}$ are denoted by $c \left( {\feature}_{d} \right)$ and $c_{g} \left( g_{j-D} \right)$, respectively. They are multiplied by their respective factors $\lambda$ and $\lambda_{g}$. The agent is penalized by $c_{\fsubset}$ if it decides to select a feature ${\feature}_d$ which has already been previously recruited into $\fsubset$. When the agent decides to make a prediction, it is penalized by $\left| \hat{y}_k - y_k \right|$ if the absolute error is larger or equal a user-determined error tolerance $\Delta$. Otherwise, the agent is not penalized. However, if the agent decides to make a prediction without recruiting any features, a penalty of $c_{g, \emptyset}$ is incurred. The chosen PM is then fitted with the entire training dataset $\left( \dataset, \labels \right)$, which is in turn used to perform an inference on a ``background'' example, comprised of average feature values $\bar{\nvec{\mathtt{X}}} = \left[ \bar{x}_{1} \ \bar{x}_{2} \ \cdots \ \bar{x}_{D}\right]$ and deliver a prediction $\hat{y}_k$.
