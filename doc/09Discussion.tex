The SMS and the Stability-SMS models perform very well ($R^2$ $> 0.8$), while the other models perform well ($R^2$ $> 0.5$). Unsurprisingly, the performances correlate strongly [Cohen \cite{cohen1988}, $p=0.08$, $r=0.71$] with the corresponding interrater reliabilities ICC\textsubscript{1.1} of the medical-board. Though the individual subscore models do not perform perfectly well, they compensate for one another when formulating the SMS. There are nonetheless, a few shortcomings and issues that should be addressed. The first is the number of features recruited by the agent to make a prediction. While penalizing the agent everytime it recruits a feature has helped produce a reduced feature subset, one can nonetheless see in Table \ref{tab:featuresRecruitedReduction} that the number of recruited features is still considerably high.
\begin{table}[H]
  \caption{Number of original features, average number of features recruited by the agent (rounded to closest integer), and the reduction in percentage.}
  \label{tab:featuresRecruitedReduction}
  \begin{tabular}{|l|l|l|l|}
    \hline
    \textbf{SMS} & \multicolumn{3}{c|}{\textbf{Number of features}} \\ \cline{2-4}
    \textbf{Subscore}  & \textbf{Original} $|\fset|$ & \textbf{Average} $|\fsubset|$ & \textbf{Reduction} $[\%]$ \\ \hline
    Trunk-SMS     & 241 & 140 & 41.9 \\ 
    Leg-SMS       & 188 & 113 & 39.9 \\ 
    Arm-SMS       & 99  & 59  & 40.4 \\
    Speed-SMS     & 31  & 21  & 32.2 \\
    Fluency-SMS   & 253 & 141 & 44.3 \\
    Stability-SMS & 238 & 133 & 44.1 \\ \hline \hline
    \multicolumn{3}{|c|}{\textbf{Average reduction of number of features}} & 40.5 \\ \hline
  \end{tabular}
\end{table}
This could be counterintuitive and pose the risk of overwhelming clinicians with information \cite{lee2020,lee2021}. It would thus be beneficial to reduce the number of recruited features to an amount, that lies within established cognitive processing limits (e.g. $7 \pm 2$) \cite{miller1994}. While this might exclude a few key features, interpretability is often prioritized over predictive accuracy in developing CDSSs, as they should serve as an aid rather than a replacement for human experts. Secondly, the agent was also observed to alternate between PM numerous times within an episode before finally deciding to make a prediction. This seems to reflect ``undecisiveness'', akin to that of a human ML practitioner. Thirdly, medical diagnostic erros are often asymmetrical. While an overpessimistic result (a false positive) may lead to unnecessary follow-up testing or temporary patient anxiety, the consequences of overoptimistic results (a false negative) can result in the catastrophic delay of life-saving treatment, or doctors recommending futile, aggressive care instead of more beneficial palliative care \cite{christakis2000}.

A key highlight of this RL-based approach is the versatility it offers by reformulating the reward function to account for these issues. For instance, to encourage the agent to limit the number of recruited features to around seven, the penalty of recruiting a feature can be progressively increased after the number of recruited features $|\fsubset|$ exceeds a user-defined threshold. If there exists a feature that possesses an undesired characteristic such as being difficult to interpret or challenging to obtain, one could also set a higher penalty for that feature. Similarly, one could also penalize the agent more for choosing a MLP, over less computationally taxing ones like SVM. To steer the agent away from making overoptimistic diagnostic errors, errors where the predicted medical score is lower than that of the true score can be weighed more heavily. Shown in (\ref{eq:HypotheticalRewardFunction}) is a hypothetical reward function, that could account for the aformentioned issues.
\begin{align}
    R(\nvec{s}, a) &=
    \begin{cases}
      -\lambda_{d} c \left( {\feature}_{d} \right) & \text{if } a = a_{d} \text{, where } 1 \leq d \leq D , \,\, \lambda_{d} \in \nvec{\lambda}_{\fset}\\
      -\lambda c^{*}(|\fsubset|) & \text{if } a = a_{d} \text{ and } |\fsubset| > |{\fsubset}^{*}| \text{, where } 1 \leq d \leq D \\
      -c_{\fsubset} & \text{if } a = a_{d} \in \fsubset \text{, where } 1 \leq d \leq D \\
      -\lambda_{j} c_{g} \left( g_{j-D} \right) & \text{if } a = a_{j} \text{, where } D+1 \leq j \leq D+|\mathcal{G}| , \,\, \lambda_{g} \in \nvec{\lambda}_{\pmset}\\
      0 & \text{if } a = a_0 \text{, with } \left| \hat{y}_k - y_k \right| < \Delta \\
      -\left| \hat{y}_k - y_k \right| & \text{if } a = a_0 \text{, with } \left| \hat{y}_k - y_k \right| \geq \Delta \text{ and } \hat{y}_k > y_k \\
      -\lambda_{y} \left| \hat{y}_k - y_k \right| & \text{if } a = a_0 \text{, with } \left| \hat{y}_k - y_k \right| \geq \Delta \text{ and } \hat{y}_k < y_k\\
      -c_{g, \emptyset} & \text{if } a = a_0 \text{, with } \fsubset = \emptyset
    \end{cases}
    \label{eq:HypotheticalRewardFunction}
\end{align}
The penalty factors for recruiting a feature and PM can be stored in their respective lookup tables $\nvec{\lambda}_{\fset} = \left[ {\lambda}_{1} \ {\lambda}_{2} \ \cdots \ {\lambda}_{d} \ \cdots \ {\lambda}_{D}\right]$ and $\nvec{\lambda}_{\pmset} = \left[ {\lambda}_{1} \ {\lambda}_{2} \ \cdots \ {\lambda}_{j} \ \cdots \ {\lambda}_{|\pmset|}\right]$, where the each penalty $\lambda$ is user-defined. To steer the agent towards selecting a user-desired number of features $|{\fsubset}^{*}|$, the penalty of recruiting a feature can be progressively increased by $c^{*}(|\fsubset|)$, as a function of the number of recruited features $|\fsubset|$. In the case of a prediction error larger than the error tolerance $\Delta$, the penalty for predicted scores larger than that of the true score $\hat{y}_k > y_k$ is the absolute difference, whereas the penalty for predicted scores smaller than the true score $\hat{y}_k < y_k$ is the absolute difference multiplied by a user-defined factor $\lambda_y$.

One limitation that should be accounted for is the unbalanced datasets on which the agent is trained on. This could especially be problematic during training because a sample is randomly chosen at every iteration. This stochastic sampling might lead to an agent that is heavily biased toward the majority class, failing to generalize to rarer but potentially more critical scenarios. However, to account for this, each sample is weighted to account for two source of biases as described in \cite{liaw2025}. A second issue would be a technical one, namely the sheer scale of the required iterations. This attached computational cost is further multiplied when one factors in hyperparameter tuning. Therefore, to effectively explore the hyperparameter space and ensure model robustness, one should leverage cloud supercomputers, that enables massive parallelization to manage the vast search space.

One lesson learned from applying DQL in balancing an inversed pendulum, as described in the earlier section, is the importance of beginning with a reasonably ``good'' initial estimate for solution convergence. Given the simple implementation described earlier, the controller would very unlikely ever swing the pendulum into its upright position, if the pendulum was initially simply hanging from the revolute joint. Similarly here, the agent would very unlikely find a good subset of features, if it was presented with all 680 features. Moreover, if the original set of features were all included, the number of selected features by the agent would be tremendously overwhelming to effectively help a clinician.
% Run a round of ltfm with all 680 feature to prove the point.
