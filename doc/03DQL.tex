To begin, the \emph{quality function}, also referred to as the \emph{action-value} function \cite{watkins1992,mnih2015,sutton2018,almahamid2021} is defined as
\begin{equation}
    \begin{split}
    Q_{\policy} (\nvec{s}, a) = R(\nvec{s}, a) + \gamma V_{\policy}(\nvec{s}')\,,
    \end{split}
    \label{eq:QualityFunction}
\end{equation}
which describes the \emph{joint-desirability} of performing the action $a$ for the given state $\nvec{s}$. Following this formulation, the agent selects the action $a$ that yields the maximum $Q$-value, for the given state $\nvec{s}$ as shown in (\ref{eq:QLearningAction})
\begin{equation}
    \policy (\nvec{s}) = \argmax_{a} Q_{\policy} (\nvec{s}, a) \, .
    \label{eq:QLearningAction}
\end{equation}
The goal in \emph{Q-Learning} \cite{watkins1992,almahamid2021} is for the agent to learn an optimal policy that maximizes the action-value function
\begin{equation}
  \begin{aligned}
    Q_{\optpolicy} (\nvec{s}, a) = \max_{\policy} Q_{\policy} (\nvec{s}, a) &= R(\nvec{s}, a) & &+ \, \gamma \max_{a'} Q_{\optpolicy} (\nvec{s}', a') \\
    &= r & &+ \, \gamma \max_{a'} Q_{\optpolicy} (\nvec{s}', a') \,,
  \end{aligned}
\end{equation}
which yields the following intuition. If the optimal value $Q_{\optpolicy} (\nvec{s}', a')$ for the state $\nvec{s}'$ at the next time-step is known for all possible actions $a'$, then the optimal strategy is to simply select the action $a'$ that maximizes the value of $r + \gamma Q_{\optpolicy} (\nvec{s}', a')$ \cite{mnih2015}.

In the original $Q$-Learning, the optimal action-value function is obtained via a value iteration algorithm, where the $Q$-values are essentially maintained in a $Q$-Table and updated iteratively \cite{watkins1992,almahamid2021}. In the seminal work by \cite{mnih2015}, Deep $Q$-Learning (DQL) was introduced, where a deep convolutional neural network was used to approximate the action-value $Q_{\policy}$ through some parameterization $\boldsymbol{\theta}$.
\begin{equation}
    Q_{\policy} (\nvec{s}, a) \approx Q_{\policy} (\nvec{s}, a; \boldsymbol{\theta}) \,.
\end{equation}
The neural network function approximator of the parameters $\boldsymbol{\theta}$ is referred to as the \emph{Q-network}, where $\boldsymbol{\theta}$ updated by minimizing the loss function (\ref{eq:DQNLossFunction})
\begin{equation}
    \min_{\boldsymbol{\theta}} \dfrac{1}{|\mathcal{B}|} \sum_{\boldsymbol{e} \in \mathcal{B}} \left[ \left( r + \gamma \max_{a'} Q_{\policy} (\nvec{s}', a'; \boldsymbol{\theta}) \right) - Q_{\policy} (\nvec{s}, a; \boldsymbol{\theta}) \right]^2 \, ,
    \label{eq:DQNLossFunction}
\end{equation}
over a batch of samples $\mathcal{B}$ \cite{mnih2015}, where each sample pertains to a sampled experience $\boldsymbol{e} = \left( \nvec{s} \, , a \, , \nvec{s}' \, , r \right)$. The term on the right is referred to as the \emph{target values} $\left( r + \gamma \max_{a'} Q_{\policy} (\nvec{s}', a'; \boldsymbol{\theta}) \right)$. To deal with the well-known instability of using deep neural networks in RL, \cite{mnih2015} introduces two key ideas, namely $(i)$ updating the neural network over \emph{randomly sampled experiences} of the agent-environment interactions and $(ii)$ only \emph{periodically updating} the neural network towards the target values. The first idea involves storing the agent's experiences $\boldsymbol{e}_t$ at each time-step $t$ into a \emph{replay memory} $D_t = \left\{ \boldsymbol{e}_1\,, \boldsymbol{e}_2\,, \cdots\,, \boldsymbol{e}_t \right\}$, from which a batch of experiences are then randomly sampled $\mathcal{B}_{D} \subseteq D$ to update the $Q$-network \cite{mnih2015}. This helps break the correlations between each consecutive experience, thus preventing undesired feedback loops during learning \cite{mnih2015}.

The second idea is implemented by using a clone of the $Q$-network, termed the \emph{target network} $\hat{Q}_{\policy}$ to generate the target values, whose parameters $\boldsymbol{\theta}^{-}$ follow the parameters $\boldsymbol{\theta}$ of the $Q$-network with a slight delay, which helps the learning better converge \cite{mnih2015}. Following the suggestion by \cite{lillicrap2015}, the parameters $\boldsymbol{\theta}^{-}$ are updated \emph{softly} according to (\ref{eq:TargetUpdates}).
\begin{equation}
  \boldsymbol{\theta}^{-} = \tau \boldsymbol{\theta} + \left(1 - \tau \right) \boldsymbol{\theta}^{-} \,,
  \label{eq:TargetUpdates}
\end{equation}
where $\tau$ denotes the \emph{soft target update rate}. The $Q$-network's weights are thereby updated according to
\begin{equation}
    \min_{\boldsymbol{\theta}} \dfrac{1}{|\mathcal{B}_{D}|} \sum_{\boldsymbol{e} \in \mathcal{B}_D} \left[ \left( r + \gamma \max_{a'} \hat{Q}_{\policy} (\nvec{s}', a'; \boldsymbol{\theta}^{-}) \right) - Q_{\policy} (\nvec{s}, a; \boldsymbol{\theta}) \right]^2 \, .
    \label{eq:DQNLossFunction2}
\end{equation} 
To promote exploration, the agent's action is selected according to an \emph{$\epsilon$-greedy} algorithm, where the parameter $\epsilon$ denotes the probability of the agent performing a random action instead of the maximizing action according to (\ref{eq:QLearningAction}) \cite{mnih2015,brunton2022}. Intuitively, as the $Q$-function improves over the course of training, $\epsilon$ should gradually decay, allowing the agent to increasingly choose the maximizing action \cite{brunton2022}. For this work, $\epsilon$ is implemented to decay exponentially according to (\ref{eq:EpsilonExpDecay})
\begin{equation}
  \epsilon = \left( {\epsilon}_{\text{initial}} - {\epsilon}_{\text{final}} \right) e^{-\frac{t_c}{{\epsilon}_{\text{decay}}}} + {\epsilon}_{\text{final}} \, ,
  \label{eq:EpsilonExpDecay}
\end{equation}
where $t_c$ denotes the cumulative time-steps over episodes, and ${\epsilon}_{\text{initial}}$, ${\epsilon}_{\text{final}}$, and ${\epsilon}_{\text{decay}}$, the initial value, final value, and decay rate of $\epsilon$, respectively.

Algorithm \ref{alg:DQN} shows how DQL is implemented in this work, combined with experience replay and an $\epsilon$-greedy algorithm for selecting actions. The $Q$-network is updated at every time-step $t$, provided the memory $D$ contains at least the number of user-specified batch size for training $|\mathcal{B}_D|$. Moreover, the memory $D$ is implemented in practice as a finite-sized cache which stores only the $N$ most recent experiences, discarding the oldest samples as new ones are added \cite{mnih2015,lillicrap2015}.
\begin{algorithm}[!t]
    \caption{DQL with experience replay, combined with an $\epsilon$-greedy algorithm for promoting random exploration \cite{mnih2015}. The notations $\nvec{s}_{k,t}$, $a_{k,t}$, $\nvec{s}_{k,t+1}$, $r_{k,t}$, and $y_{k,t}$ denote the state, action, next state, reward, and target values of the $k$-th episode, at time-step $t$ respectively.}
    \label{alg:DQN}
    \begin{algorithmic}
      \State Initialize number of episodes $K$
      \State Initialize replay memory $D$ with capacity $N$
      \State Initialize discount rate $\gamma$
      \State Initialize batch size $|\mathcal{B}_{D}|$ for updating parameters $\boldsymbol{\theta}$
      \State Initialize $Q$-network $Q_{\policy}$ with random weights $\boldsymbol{\theta}$
      \State Initialize target network $\hat{Q}_{\policy}$ with weights $\boldsymbol{\theta}^{-} = \boldsymbol{\theta}$
      \State Initialize soft target update rate $\tau$
      \State Initialize $\epsilon$ with parameters ${\epsilon}_{\text{initial}}$, ${\epsilon}_{\text{final}}$, and ${\epsilon}_{\text{decay}}$ for random exploration
      \State Initialize counter for cumulative time-steps over episodes $t_c = 1$
      \For{$k := 1$ to $K$} \Comment for each $k$-th episode
          \State Initialize time-step $t=1$
          \State Initialize initital state $\nvec{s}_{k,t=1}$
          \While{$T_{\text{end}}$ is false} \Comment termination condition for $k$-th episode not fulfilled
              \State With probability of $\epsilon$ select a random action $a_{k,t}$,
              \State $\quad$ otherwise $a_{k,t} = \max_{a_{k,t}} Q(\nvec{s}_{k,t}, a_{k,t})$
              \State Execute action $a_{k,t}$, and observe reward $r_{k,t}$ and next state $\nvec{s}_{k,t+1}$
              \State Store episode $\boldsymbol{e}_{k,t} = \left( \nvec{s}_{k,t} \, , a_{k,t} \, , \nvec{s}_{k,t+1} \, , r_{k,t} \right)$ in replay memory $D$
              \If{$|D| \geq |\mathcal{B}_{D}|$} \Comment if number of stored experiences are at least batch size
                  \State Sample minibatch of random episodes $\boldsymbol{e}_{j} = \left( \nvec{s}_{j} \, , a_{j} \, , \nvec{s}_{j} \, , r_{j} \right)$ from $D$
                  \If{$T_{end}$ is true} \Comment termination condition for $k$-th episode fulfilled
                      \State $y_{j} = r_{j}$
                  \Else
                      \State $y_{j} = r_{j} + \gamma \max_{a_{j}'} \hat{Q}_{\policy} (\nvec{s}_{j}', a_{j}'; \boldsymbol{\theta}^{-})$
                  \EndIf
                  \State Perform a gradient descent step on $\left( y_{j} - Q_{\policy} (\nvec{s}_j, a_j; \boldsymbol{\theta}) \right)^2$
                  \State $\quad$ with respect to $Q$-network parameters $\boldsymbol{\theta}$
              \EndIf
              \State Update parameters of target network $\boldsymbol{\theta}^{-}$ towards $\boldsymbol{\theta}$ according to (\ref{eq:TargetUpdates})
              \State Update $\epsilon$ according to (\ref{eq:EpsilonExpDecay})
              \State Update time-step counter $t = t + 1$
              \State Update cumulative time-step counter $t_c = t_c + 1$
          \EndWhile
      \EndFor
    \end{algorithmic}
\end{algorithm}
