RL is a branch of machine learning that deals with learning control laws and policies to interact with an environment, from experience \cite{brunton2022}. In contrast to supervised learning, RL does not require a training dataset to learn, but rather an environment to interact with and via trial-and-error, learn the best course of action to achieve a long-term objective \cite{mnih2015,sutton2018}. The learning is guided via feedback, or formally \emph{reinforcement}, a concept biologically inspired from the study of animal psychology, where animals have been observed to be ``hardwired'' to recognize pain and hunger as negative rewards, and food intake as positive rewards, and thus, mold their behavior accordingly to best maximize (positive) rewards \cite{pavlov2010,brunton2022}. As shown in Figure \ref{fig:RLSchematic}, an agent senses the \emph{state} of its \emph{environment}, and learns to take \emph{actions} that maximizes cumulative future rewards. Specifically, the agent arrives at a sequence of different states $\nvec{s}$ by performing actions $a$, which either lead to positive or negative rewards $R$ used for learning \cite{brunton2022}.
\begin{figure}[h!]
  \centering
  \begin{overpic}[width=1.0\columnwidth]{ReinforcementLearning.eps}
    % Nouns
    \put(66, 33){\emph{ENVIRONMENT}}
    \put(8, 26){\emph{AGENT}}
    \put(1.3, 17){\emph{STATE}, $\nvec{s}$}
    \put(38, 27){\emph{POLICY}}
    \put(39, 25){$\policy$}
    \put(35, 38){\emph{REWARD}, $R$}
    % Verbs
    \put(40, 1){Observe \emph{STATE}, $\nvec{s}$}
    \put(51.5, 23){Perform}
    \put(51.5, 20){\emph{ACTION}, $a$}
    % Variables
    \put(19, 24){$\dot{x}$}
    \put(19, 20){$x$}
    \put(19, 16){$\varphi$}
    \put(19, 12){$\dot{\varphi}$}
    % Environment
    \put(96, 9){$x$}
    \put(71, 30){$y$}
    \put(77, 4){$x$}
    \put(89.8, 4.5){$\dot{x}$}
    \put(76, 11){$F$}
    \put(44, 19.5){$+F$}
    \put(44, 14){$-F$}
    \put(90, 28){$\varphi$}
    \put(95, 21){$\dot{\varphi}$}
    \put(72, 21){$\nvec{s}=\begin{bmatrix}x \\ \dot{x} \\ \varphi \\ \dot{\varphi}\end{bmatrix}$}
    % Parameters
    
  \end{overpic}
  \caption{Schematic of RL, where an agent senses its environmental state $\nvec{s}$ and performs an action $a$, according to a policy $\policy$ that is optimized through learning to maximize cumulative future rewards $R$. In recent works, a typical approach to represent the policy $\policy$ is to use a deep neural network. Such a policy is known as a \emph{deep policy network}. Figure adapted from \cite{brunton2022}.}
  \label{fig:RLSchematic}
\end{figure}