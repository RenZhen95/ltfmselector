The dataset used for modeling consists of 100 hemiparetic stroke patients which received a clinical examination and a full-body instrumented gait analysis. An interdisciplinary board of medical experts assigned each patient a Stroke Mobility Score (SMS) \cite{raab2020}, a multiple-cue clinical observational score comprised of six sub-scores each pertaining to a functional criterion of gait. The medical-board's gait-assessments (see Figure \ref{fig:LTFMvsBoard} were computed at subscore level as the mode of all individual recommendations. If the mode could not be defined, the subscore not in contention for the highest count was used as a tiebreaker. From the measurements, 904 measured stride pairs of 100 patients were obtained, 680 gait features extracted, and the dataset split 70/30 for training and testing. As a preprocessing step, expert knowledge was used to trim the features accordingly, followed by filtering out statistically non-discriminatory features. Detailed descriptions of these steps can be found in \cite{liaw2025}.

For each SMS subscore, the agent was trained on a batch size $|\mathcal{B}_{D}|=256$, sampled from the replay memory that stores the last $N=10000$ experiences, over 1500 episodes. The reward function was formulated with the penalties $c \left( {\feature}_d \right) = c_{g} \left( g_{j-D} \right) = 0.01$ and their respective factors $\lambda = \lambda_{g} = 1.0$, $c_{\fsubset} = 1.0$, $c_{g, \emptyset} = 5.0$, and $\Delta = 0.5$. The set $\mathcal{G}$ included random forest (RF), support vector machine (SVM), and multilayer perceptron (MLP) regression models. The RF and SVM regression models were implemented using scikit-learn \cite{scikit}, while the MLP regression model was similarly implemented using PyTorch \cite{pytorch}.

The optimal hyperparameters of each PM are selected by evaluating each hyperparameter combination (grid search) with a 3-fold cross-validation to select the hyperparameters that maximize the cross-validation estimate of the $R^2$. The SVM was implemented with a radial basis function as the kernel and grid-searched across the regularization parameter $C=\left\{ 0.1 \,, 0.5 \,, 1.0\right\}$, and the RF with 100 estimators and grid-searched across the maximum depth of trees $\left\{ 3 \,, 4 \,, 5 \right\}$. The MLP was implemented with four hidden layers with the number of units $\left\{ 32 \,, 32 \,, 16 \,, 8 \right\}$, trained with a batch size of 128 across 300 training epochs, AdamW learning rate of \num{1e-4}, and grid-searched across the weight decay $\lambda = \left\{ 0.01 \,, 0.1 \right\}$ and dropout probability $\left\{ 0.01 \,, 0.05 \right\}$.

The remaining hyperparameters not mentioned are left in their default values as implemented in scikit-learn and PyTorch. The policy and target networks were implemented as MLPs with two hidden layers, each with 1024 neurons, with the discount rate $\gamma = 0.99$ and soft update rate learning rate $\tau = $ \num{5e-4}. The parameters of the $Q$-network are optimized using an AdamW optimizer with the rate $l_r = $ \num{1e-5}. For the $\epsilon$-greedy algorithm, the agent selects a random action with the probability $\epsilon = \left( 0.9 - 0.05 \right) e^{-\frac{t_c}{1000}} + 0.05$. The algorithm was implemented using PyTorch \cite{pytorch}, alongside other standard Python packages such as NumPy \cite{numpy} and pandas \cite{pandas}. The training was performed on a computing system equipped with a 3.6 GHz AMD Ryzen\texttrademark{} 7 3700X CPU and an NVIDIA\textsuperscript{\textregistered{}} GeForce\textsuperscript{\textregistered{}} GTX 1650 GPU.
