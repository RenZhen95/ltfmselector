The dataset used for modeling consists of 100 hemiparetic stroke patients which received a clinical examination and a full-body instrumented gait analysis. An interdisciplinary board of medical experts assigned each patient a Stroke Mobility Score (SMS) \cite{raab2020}, a multiple-cue clinical observational score comprised of six sub-scores each pertaining to a functional criterion of gait. The medical-board's gait-assessments (see Figure \ref{fig:LTFMvsBoard} were computed at subscore level as the mode of all individual recommendations. If the mode could not be defined, the subscore not in contention for the highest count was used as a tiebreaker. From the measurements, 904 measured stride pairs of 100 patients were obtained, 680 gait features extracted, and the dataset split 70/30 for training and testing. As a preprocessing step, expert knowledge was used to trim the features accordingly, followed by filtering out statistically non-discriminatory features. Detailed descriptions of these steps can be found in \cite{liaw2025}.

For each SMS-subscore, the agent was trained according to the DQL algorithm described earlier, with the environmental parameters and agent hyperparameters initialized with values as shown in Table \ref{tab:ltfmHyperparameters}. The set $\mathcal{G}$ included RF, SVM, and MLP regression models. The RF and SVM regression models were implemented using scikit-learn \cite{scikit}, while the MLP regression model was similarly implemented using PyTorch \cite{pytorch}. The optimal hyperparameters of each PM are selected by evaluating each hyperparameter combination (grid search) with a 3-fold cross-validation to select the hyperparameters that maximize the cross-validation estimate of the $R^2$. The combination of hyperparameters grid-searched are as shown in Table \ref{tab:pmHyperparameters}. The remaining hyperparameters, not included in Table \ref{tab:pmHyperparameters} are left in their default values as implemented in scikit-learn and PyTorch.

The policy and target networks were implemented as MLPs with two hidden layers, each with 1024 neurons. The agent is trained over 1500 episodes, where the parameters $\boldsymbol{\theta}$ of the $Q$-network are optimized using an AdamW optimizer \cite{loshchilov2017}, with the learning rate $l_r$. The algorithm was implemented mainly using PyTorch \cite{pytorch}, alongside other standard Python packages such as NumPy \cite{numpy} and pandas \cite{pandas}. The training was performed on a computing system equipped with a 3.6 GHz AMD Ryzen\texttrademark{} 7 3700X CPU and an NVIDIA\textsuperscript{\textregistered{}} GeForce\textsuperscript{\textregistered{}} GTX 1650 GPU.
\begin{table}[H]
  \centering
  \begin{tabular}{c|p{0.35\textwidth}p{0.35\textwidth}}
    \hline
    \textbf{PM} & \textbf{Fixed Hyperparameters} & \textbf{Tuned Hyperparameters} \\
    \hline
    \multirow{2}{*}{\makebox[1em]{\rotatebox{90}{SVM \hspace{0.1em}}}} 
    & $\cdot$ kernel: radial basis function & $\cdot$ regularization parameter $C$ \\
    &  & $\qquad = \left[ 0.1 \,, 0.5 \,, 1.0 \right]$ \\
    \hline
    \multirow{2}{*}{\makebox[1em]{\rotatebox{90}{RF \hspace{0.2em}}}}
    & $\cdot$ number of estimators: 100 & $\cdot$ maximum depth of trees \\
    &  & $\qquad = \left[ 3 \,, 4 \,, 5 \right]$ \\
    \hline
    \multirow{5}{*}{\makebox[1em]{\rotatebox{90}{MLP}}}
    & $\cdot$ 4 hidden layers with & $\cdot$ weight decay $\lambda$ \\
    & $\qquad$ units $\left\{ 32 \,, 32 \,, 16 \,, 8 \right\}$ & $\qquad = \left[ 0.01 \,, 0.1 \right]$ \\
    & $\cdot$ epochs: 300 & $\cdot$ dropout probability \\
    & $\cdot$ batch size: 128 & $\qquad \left[ 0.01 \,, 0.05 \right]$ \\
    & $\cdot$ learning rate: $1 \times 10^{-4}$ & \\
    \hline
  \end{tabular}
  \caption{The PMs included in the set $\mathcal{G}$ and their fixed and tuned hyperparameters, evaluated via a 3-fold cross-validation procedure. Other hyperparameters not included in this table are left in their default values, as implemented in scikit-learn and PyTorch.}
  \label{tab:pmHyperparameters}
\end{table}

\begin{table}[H]
  \centering
  \begin{tabular}{p{0.65\textwidth}p{0.09\textwidth}p{0.1\textwidth}}
    \hline
    \multicolumn{3}{l}{\textbf{Environmental Parameters}} \\
    \hline
    Penalty of recruiting a feature ${\feature}_{d}$ & $c \left( {\feature}_d \right)$ & 0.01 \\
    Penalty of recruiting a feature ${\feature}_{d} \in \fsubset$ & $c_{\fsubset}$ & 1.0 \\
    Penalty of choosing a PM & $c_{g} \left( g_{j-D} \right)$ & 0.01 \\
    Penalty factor of recruiting features & $\lambda$ & 1.0 \\
    Penalty factor of choosing a PM & $\lambda_{g}$ & 1.0 \\
    Penalty of choosing to predict with $\fsubset = \emptyset$ & $c_{g, \emptyset}$ & 5.0 \\
    Error tolerance & $\Delta$ & 0.5 \\
    \hline
    \multicolumn{3}{l}{\textbf{Agent Hyperparameters}} \\
    \hline
    Number of episodes & $K$ & 1500 \\
    Number of experiences stored in replay memory & $N$ & 10000 \\
    Discount rate & $\gamma$ & 0.99 \\
    Batch size of experiences drawn from replay memory & $|\mathcal{B}_{D}|$ & 256 \\
    Learning rate of AdamW optimizer & $l_r$ & \num{1e-5} \\
    Soft target update rate & $\tau$ & \num{5e-4} \\
    Initial probability $\epsilon$ for random exploration & $\epsilon_{\text{initial}}$ & 0.9 \\
    Final probability $\epsilon$ for random exploration & $\epsilon_{\text{final}}$ & 0.05 \\
    Decay rate of probability $\epsilon$ for random exploration & $\epsilon_{\text{decay}}$ & 1000 \\
    \hline
  \end{tabular}
  \caption{Environmental parameters for patient-specific feature and model selection, and the learning hyperparameters of the DQL agent.}
  \label{tab:ltfmHyperparameters}
\end{table}
% \begin{table}
% \caption{Table captions should be placed above the
% tables.}\label{tab1}
% \begin{tabular}{|l|l|l|}
% \hline
% Heading level &  Example & Font size and style\\
% \hline
% Title (centered) &  {\Large\bfseries Lecture Notes} & 14 point, bold\\
% 1st-level heading &  {\large\bfseries 1 Introduction} & 12 point, bold\\
% 2nd-level heading & {\bfseries 2.1 Printing Area} & 10 point, bold\\
% 3rd-level heading & {\bfseries Run-in Heading in Bold.} Text follows & 10 point, bold\\
% 4th-level heading & {\itshape Lowest Level Heading.} Text follows & 10 point, italic\\
% \hline
% \end{tabular}
% \end{table}
