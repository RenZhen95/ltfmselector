The environment is represented by the state $\nvec{s}_t$ at the current time-step $t$. The agent performs an action $a_t$ according to a learned policy $\policy$, which results in the current state $\nvec{s}_t$ evolving to the next state $\nvec{s}_{t+1}$. Consequently, the agent receives an appropriate reward $R_{t+1} \in \mathbb{R}$ one time-step later \cite{sutton2018,brunton2022}. These collectively form an \emph{experience} $\boldsymbol{e}_t$, which describes the knowledge an agent has amassed from interacting with the environment, usually expressed as a tuple $\boldsymbol{e}_t = \left( \nvec{s}_t \, , a_t \, , \nvec{s}_{t+1} \, , R_{t+1} \right)$ \cite{almahamid2021}. The agent-environment interaction thereby yields a \emph{trajectory}, as shown in (\ref{eq:Trajectory}) \cite{sutton2018,brunton2022}
\begin{equation}
  \begin{aligned}
    \nvec{s}_0 \,, a_0 \,, R_1 \,, & & \nvec{s}_1 \,, a_1 \,, R_2 \,, & & \nvec{s}_2 \,, a_2 \,, R_3 \,, \cdots \,, \\
    \boldsymbol{e}_0 \,, & & \boldsymbol{e}_1 \,, & & \boldsymbol{e}_2 \,, \cdots \,.
  \end{aligned}
  \label{eq:Trajectory}
\end{equation}
Formally described, the environment evolves according to a \emph{Markov decision process}, where the random variables $\nvec{s}_{t} \in \mathcal{S}$ and $R_t \in \mathcal{R}$ each have defined discrete probability distributions, that depend only on the preceding state and action \cite{sutton2018,brunton2022}. They evolve according to (\ref{eq:MDPDynamics}) and (\ref{eq:RewardFunction}), respectively.
\begin{equation}
    P(\nvec{s}', r \,|\, \nvec{s}, a) = \mathrm{Pr}\left\{ \nvec{s}_{t+1} = \nvec{s}' , R_{t+1} = r \,|\, \nvec{s}_t = s, a_t = a \right\}
    \label{eq:MDPDynamics}
\end{equation}
\begin{equation}
    r(\nvec{s}, a) = \mathbb{E} \left[ R_{t+1} \,|\, \nvec{s}_{t}=\nvec{s}, a_t = a \right] = \sum_{r \in \mathcal{R}} r \sum_{\nvec{s}' \in \mathcal{S}} P(\nvec{s}', r \,|\, \nvec{s}, a)
    \label{eq:RewardFunction}
\end{equation}
In short, the goal of RL is to maximize the expected discounted \emph{return} $G_{t} = R_{t+1} + \gamma R_{t+2} + \gamma^{2} R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^{k} R_{t+1+k}$, where $\gamma$ denotes the \emph{discount rate} \cite{sutton2018,brunton2022}. In episodic environments, each episode ends in a special \emph{terminal state}, when the agent encounters a \emph{termination condition} $T_{\text{end}}$ \cite{almahamid2021}. This is followed by a reset to a standard or random starting state, and the agent begins with the next episode. To learn a policy that maximizes the return $G$, the ``desirability'' of being in a given state $\nvec{s}_t$ is quantified via the \emph{value function} (\ref{eq:ValueFunction})
\begin{equation}
    \begin{split}
      V_{\policy} (\nvec{s}) &= \mathbb{E}_{\policy} \left[ G_{t} \left. \, \right\rvert \, \nvec{s}_t = \nvec{s} \right] \\
                            &= \mathbb{E}_{\policy} \left[ R_{t+1} + \gamma G_{t+1} \left. \, \right\rvert \, \nvec{s}_t = \nvec{s} \right] \\
                             &= \sum_{a} \policy(a \,|\, \nvec{s}) \sum_{\nvec{s}'} \sum_{r} P(\nvec{s}', r \,|\, \nvec{s}, a) \left[ r + \gamma\mathbb{E}_{\policy} \left[ G_{t+1}|\nvec{s}_{t+1}=\nvec{s}' \right] \right] \\
                             &= \sum_{a} \policy(a \,|\, \nvec{s}) \sum_{\nvec{s}', r} P(\nvec{s}', r \,|\, \nvec{s}, a) \left[ r + \gamma V_{\policy}(\nvec{s}') \right] \,,
                               \text{ for all } \nvec{s} \in \mathcal{S} \, ,
    \end{split}
    \label{eq:ValueFunction}
\end{equation}
which describes the expected discounted return when starting from $\nvec{s}$, and following $\policy$ thereafter \cite{sutton2018}. The optimal policy can thus be rewritten in terms of the value function as 
\begin{equation}
    \begin{split}
        V_{\optpolicy} (\nvec{s}) &= \max_{a} \mathbb{E} \left[R_{t+1} + \gamma V_{\optpolicy}(\nvec{s}') \, \rvert \, \nvec{s}_{t}=\nvec{s} \,, a_{t}=a \right] \\
                                  &= \max_{a} \sum_{\nvec{s}', r} P(\nvec{s}', r \,|\, \nvec{s}, a) \left[ r + \gamma V_{\optpolicy}(\nvec{s}') \right] \,.
    \end{split}
    \label{eq:BellmanEq}
\end{equation}
Equation (\ref{eq:BellmanEq}) is known as the \emph{Bellman equation}, which possesses an important property. It can be recursively broken down for every subsequence of steps, which implies that a control policy for a multi-step procedure must also be locally optimal for every subsequence of steps. This allows for solving a large optimization problem by locally optimizing every subsequence \cite{bellman1966,sutton2018,brunton2022}. The discount factor $\gamma$ guides the agent towards learning a behavior that balances the trade-off between immediate gratification and long-term strategic gains \cite{sutton2018}. Taking chess for example, the agent is willing to make sacrifices that result in temporary unfavorable positions, in order to achieve the ultimate goal of checkmating the opponent \cite{huegle2022}. 

Classically, the value function $V$ is computed iteratively, and used to search for better policies via methods of dynamic programming \cite{sutton2018}. Classical dynamic programming is however of limited utility for two main reasons \cite{sutton2018,brunton2022}, namely $(i)$ the assumption of a perfect model, i.e. \emph{a priori} knowledge of the environmental transition dynamics $P(\nvec{s}, r \,|\, \nvec{s}, a)$, and $(ii)$ memory and computational constraints for handling large and combinatorial state spaces. One approach in dealing with this issue is to apply a function approximator to model the value function based on gathered experiences \cite{sutton2018,mnih2015}. There exist various RL algorithms in the literature \cite{almahamid2021}, each suited for different environment types. The choice of RL algorithm depends namely on the $(i)$ number of states and $(ii)$ action types. For this work involving an environment that comprises an $(i)$ \emph{unlimited} number of states, $(ii)$ an agent that performs \emph{discrete} actions, and $(iii)$ no a priori knowledge of the environment dynamics, the DQL algorithm by \cite{mnih2015} is best suited. The equations in the next sections will be formulated deterministically (i.e. $P(\nvec{s}, r \,|\, \nvec{s}, a)=1$).
